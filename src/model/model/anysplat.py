import os
from copy import deepcopy
import time
from typing import Optional
from einops import rearrange
import huggingface_hub
from omegaconf import DictConfig, OmegaConf
import torch
import torch.distributed
import torch.nn as nn
import numpy as np
import torch.nn.functional as F
from dataclasses import dataclass
from src.dataset.types import BatchedExample
from src.model.encoder.common.gaussian_adapter import GaussianAdapterCfg
from src.model.decoder.decoder_splatting_cuda import DecoderSplattingCUDA, DecoderSplattingCUDACfg
from src.model.encoder.anysplat import EncoderAnySplat, EncoderAnySplatCfg, OpacityMappingCfg

class AnySplat(nn.Module, huggingface_hub.PyTorchModelHubMixin):
    def __init__(
        self,
        encoder_cfg: EncoderAnySplatCfg,
        decoder_cfg: DecoderSplattingCUDACfg,
    ):  
        super(AnySplat, self).__init__()
        self.encoder_cfg = encoder_cfg
        self.decoder_cfg = decoder_cfg
        self.build_encoder(encoder_cfg)
        self.build_decoder(decoder_cfg)

    def convert_nested_config(self, cfg_dict: dict, target_class: type):
        """Convert nested dictionary config to dataclass instance
        
        Args:
            cfg_dict: Configuration dictionary or already converted object
            target_class: Target dataclass type to convert to
            
        Returns:
            Instance of target_class
        """
        if isinstance(cfg_dict, dict):
            # Convert dict to dataclass
            return target_class(**cfg_dict)
        elif isinstance(cfg_dict, target_class):
            # Already converted, return as is
            return cfg_dict
        elif cfg_dict is None:
            # Handle None case
            return None
        else:
            raise ValueError(f"Cannot convert {type(cfg_dict)} to {target_class}")

    def convert_config_recursively(self, cfg_obj, conversion_map: dict):
        """Convert nested configurations recursively using a conversion map
        
        Args:
            cfg_obj: Configuration object to convert
            conversion_map: Dict mapping field names to their target classes
                           e.g., {'gaussian_adapter': GaussianAdapterCfg}
        
        Returns:
            Converted configuration object
        """
        if not hasattr(cfg_obj, '__dict__'):
            return cfg_obj
            
        cfg_dict = cfg_obj.__dict__.copy()
        
        for field_name, target_class in conversion_map.items():
            if field_name in cfg_dict:
                cfg_dict[field_name] = self.convert_nested_config(
                    cfg_dict[field_name], 
                    target_class
                )
        
        # Return new instance of the same type
        return type(cfg_obj)(**cfg_dict)

    def convert_encoder_config(self, encoder_cfg: EncoderAnySplatCfg) -> EncoderAnySplatCfg:
        """Convert all nested configurations in encoder_cfg"""
        conversion_map = {
            'gaussian_adapter': GaussianAdapterCfg,
            'opacity_mapping': OpacityMappingCfg,
        }
        
        return self.convert_config_recursively(encoder_cfg, conversion_map)

    def build_encoder(self, encoder_cfg: EncoderAnySplatCfg):
        # Convert nested configurations using the helper method
        encoder_cfg = self.convert_encoder_config(encoder_cfg)
        self.encoder = EncoderAnySplat(encoder_cfg)

    def build_decoder(self, decoder_cfg: DecoderSplattingCUDACfg):
        self.decoder = DecoderSplattingCUDA(decoder_cfg)
    
    @torch.no_grad()
    def inference(self,
        context_image: torch.Tensor,
    ):
        self.encoder.distill = False
        encoder_output = self.encoder(context_image, global_step=0, visualization_dump=None)
        gaussians, pred_context_pose = encoder_output.gaussians, encoder_output.pred_context_pose
        return gaussians, pred_context_pose
    
    def forward(self, 
        batch: BatchedExample,
        global_step: int = 0,
        visualization_dump: Optional[dict] = None,
        near: float = 0.01,
        far: float = 100.0,
        wide_fov: bool = False,
        new_width: Optional[int] = None,
        # current_timeFrame_flag: bool = True
    ):
        # print(batch["context"]["image"].shape) torch.Size([b, 3, 3, 294, 518])
        b, v, c, h, w = batch["context"]["image"].shape
        # b, v, c, h, w = context_image.shape
        device = batch["context"]["image"].device
        encoder_output = self.encoder(batch, global_step, visualization_dump=visualization_dump)
        gaussians, pred_context_pose = encoder_output.gaussians, encoder_output.pred_context_pose
        
        if wide_fov and new_width is not None:
            ### add for wide fov rendering
            # 1. 准备新的内参矩阵
            # new_pred_all_intrinsic = pred_context_pose["intrinsic"].clone()
            # 2. 计算宽度比例
            width_scale = w / new_width
            # 3. 只修改归一化内参的 fx 部分
            # new_pred_all_intrinsic[..., 0, 0] 是 fx_norm
            pred_context_pose["intrinsic"][..., 0, 0] = pred_context_pose["intrinsic"][..., 0, 0] * width_scale
            w = new_width
            # print("intrinsic_newCalculate:", intrinsic_newCalculate)
            # print("new_pred_all_intrinsic:", new_pred_all_intrinsic)
            # print("extrinsic_newCalculate:", extrinsic_newCalculate)
            # print("pred_context_pose['extrinsic']:", pred_context_pose['extrinsic'])
        # if current_timeFrame_flag: # 当前帧是需要动静部分的
        output = self.decoder.forward(
            gaussians,
            pred_context_pose['extrinsic'],
            pred_context_pose["intrinsic"],
            torch.ones(1, v, device=device) * near,
            torch.ones(1, v, device=device) * far,
            (h, w),
            "depth",
        )      

        return encoder_output, output
    
